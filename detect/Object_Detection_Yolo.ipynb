{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patrick013/Object-Detection---Yolov3/blob/master/Object_Detection_Yolo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2cbWzUGGak7u"
      },
      "source": [
        "# Object Detection with Yolov3\n",
        "\n",
        "\n",
        "![cover](https://bitmovin.com/wp-content/uploads/2019/08/Object_detection_Blog_Image_Q3_19.jpg)\n",
        "\n",
        "Object detection is a computer vision task that involves both localizing one or more objects within an image and classifying each object in the image.\n",
        "\n",
        "It is a challenging computer vision task that requires both successful object localization in order to locate and draw a bounding box around each object in an image, and object classification to predict the correct class of object that was localized.\n",
        "Yolo is a faster object detection algorithm in computer vision and first described by Joseph Redmon, Santosh Divvala, Ross Girshick and Ali Farhadi in ['You Only Look Once: Unified, Real-Time Object Detection'](https://arxiv.org/abs/1506.02640)\n",
        "\n",
        "This notebook implements an object detection based on a pre-trained model - [YOLOv3 Pre-trained Weights (yolov3.weights) (237 MB)](https://pjreddie.com/media/files/yolov3.weights).  The model architecture is called a “DarkNet” and was originally loosely based on the VGG-16 model. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F6s9X5c_0kHg"
      },
      "source": [
        "## Load Pretrained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Kyn266nShw1y"
      },
      "outputs": [],
      "source": [
        "import struct\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import ZeroPadding2D\n",
        "from tensorflow.keras.layers import UpSampling2D\n",
        "from tensorflow.keras.layers import add, concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import pyplot\n",
        "from matplotlib.patches import Rectangle\n",
        "from numpy import expand_dims\n",
        "from src.loadweights import WeightReader  # Adjust this based on your project structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "-oqGirIFcsd2"
      },
      "outputs": [],
      "source": [
        "# Load the model architecture (make sure you import the model definition function)\n",
        "\n",
        "# Create the YOLOv3 model\n",
        "model = 'model/yolov3.h5' # This should return the model object, not a string\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "VbUr46tIdGaw",
        "outputId": "99d230b1-73cf-4124-dfe3-cd2dfadd386b"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'summary'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m()\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'summary'"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6oo2yI1804Q0"
      },
      "source": [
        "## Make Preditions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "dZojkco5A4vz"
      },
      "outputs": [],
      "source": [
        "labels = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\",\n",
        "\t\"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\",\n",
        "\t\"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\",\n",
        "\t\"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\",\n",
        "\t\"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\",\n",
        "\t\"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\",\n",
        "\t\"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\",\n",
        "\t\"chair\", \"sofa\", \"pottedplant\", \"bed\", \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\",\n",
        "\t\"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\",\n",
        "\t\"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "bluB5mi6iDTX"
      },
      "outputs": [],
      "source": [
        "IMAGE_WIDTH=416\n",
        "IMAGE_HEIGHT=416\n",
        "def load_and_preprocess_image(path,shape):\n",
        "  image=tf.io.read_file(path)\n",
        "  width,height=load_img(path).size\n",
        "  image=tf.image.decode_jpeg(image,channels=3)\n",
        "  image=tf.image.resize(image, shape)\n",
        "  image/=255\n",
        "  return image,width,height"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wmoADjikvoEv"
      },
      "source": [
        "Image comes from (https://www.strathcona.ca/transportation-roads/traffic/traffic-signals/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "colab_type": "code",
        "id": "3GdnPZLnjECD",
        "outputId": "8a44c90a-de95-498d-cd80-ca6374a247cf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:1: SyntaxWarning: invalid escape sequence '\\k'\n",
            "<>:1: SyntaxWarning: invalid escape sequence '\\k'\n",
            "C:\\Users\\kamal\\AppData\\Local\\Temp\\ipykernel_24356\\1625705379.py:1: SyntaxWarning: invalid escape sequence '\\k'\n",
            "  photo_filename='images\\kangaroo.jpg'\n"
          ]
        },
        {
          "ename": "NotFoundError",
          "evalue": "{{function_node __wrapped__ReadFile_device_/job:localhost/replica:0/task:0/device:CPU:0}} NewRandomAccessFile failed to Create/Open: images\\kangaroo.jpg : The system cannot find the file specified.\r\n; No such file or directory [Op:ReadFile]",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m photo_filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mkangaroo.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m _image, image_w, image_h\u001b[38;5;241m=\u001b[39m\u001b[43mload_and_preprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphoto_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[43mIMAGE_WIDTH\u001b[49m\u001b[43m,\u001b[49m\u001b[43mIMAGE_HEIGHT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(_image)\n",
            "Cell \u001b[1;32mIn[7], line 4\u001b[0m, in \u001b[0;36mload_and_preprocess_image\u001b[1;34m(path, shape)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_and_preprocess_image\u001b[39m(path,shape):\n\u001b[1;32m----> 4\u001b[0m   image\u001b[38;5;241m=\u001b[39m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m   width,height\u001b[38;5;241m=\u001b[39mload_img(path)\u001b[38;5;241m.\u001b[39msize\n\u001b[0;32m      6\u001b[0m   image\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mdecode_jpeg(image,channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
            "File \u001b[1;32me:\\ECS\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\io_ops.py:134\u001b[0m, in \u001b[0;36mread_file\u001b[1;34m(filename, name)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mio.read_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mio.read_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread_file\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_file\u001b[39m(filename, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     99\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Reads the contents of file.\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m  This operation returns a tensor with the entire contents of the input\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m    A tensor of dtype \"string\", with the file contents.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_io_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32me:\\ECS\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py:612\u001b[0m, in \u001b[0;36mread_file\u001b[1;34m(filename, name)\u001b[0m\n\u001b[0;32m    610\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    611\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 612\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mread_file_eager_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_SymbolicException:\n\u001b[0;32m    615\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
            "File \u001b[1;32me:\\ECS\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py:635\u001b[0m, in \u001b[0;36mread_file_eager_fallback\u001b[1;34m(filename, name, ctx)\u001b[0m\n\u001b[0;32m    633\u001b[0m _inputs_flat \u001b[38;5;241m=\u001b[39m [filename]\n\u001b[0;32m    634\u001b[0m _attrs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 635\u001b[0m _result \u001b[38;5;241m=\u001b[39m \u001b[43m_execute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mReadFile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_inputs_flat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_attrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _execute\u001b[38;5;241m.\u001b[39mmust_record_gradient():\n\u001b[0;32m    638\u001b[0m   _execute\u001b[38;5;241m.\u001b[39mrecord_gradient(\n\u001b[0;32m    639\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReadFile\u001b[39m\u001b[38;5;124m\"\u001b[39m, _inputs_flat, _attrs, _result)\n",
            "File \u001b[1;32me:\\ECS\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;31mNotFoundError\u001b[0m: {{function_node __wrapped__ReadFile_device_/job:localhost/replica:0/task:0/device:CPU:0}} NewRandomAccessFile failed to Create/Open: images\\kangaroo.jpg : The system cannot find the file specified.\r\n; No such file or directory [Op:ReadFile]"
          ]
        }
      ],
      "source": [
        "photo_filename='images/kangaroo.jpg'\n",
        "_image, image_w, image_h=load_and_preprocess_image(photo_filename,[IMAGE_WIDTH,IMAGE_HEIGHT])\n",
        "plt.imshow(_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "vZzEwLnFj6cm",
        "outputId": "c89b492b-760d-4dcd-ec68-12cbc1224c88"
      },
      "outputs": [],
      "source": [
        "image = expand_dims(_image, 0)\n",
        "yhat = model.predict(image)\n",
        "print([a.shape for a in yhat])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kLIu0OAP1ONr"
      },
      "source": [
        "## Process output matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "oNFnKYr6uMAI"
      },
      "outputs": [],
      "source": [
        "# This cell is based on https://machinelearningmastery.com/how-to-perform-object-detection-with-yolov3-in-keras/\n",
        "class BoundBox:\n",
        "\t'''\n",
        "\tObjects of boxes. (xmin,ymin) represents the upleft coordinate of the box while (xmax,ymax) means downright one.\n",
        "\t'''\n",
        "\tdef __init__(self, xmin, ymin, xmax, ymax, objness = None, classes = None):\n",
        "\t\tself.xmin = xmin\n",
        "\t\tself.ymin = ymin\n",
        "\t\tself.xmax = xmax\n",
        "\t\tself.ymax = ymax\n",
        "\t\tself.objness = objness\n",
        "\t\tself.classes = classes\n",
        "\t\tself.label = -1\n",
        "\t\tself.score = -1\n",
        " \n",
        "\tdef get_label(self):\n",
        "\t\tif self.label == -1:\n",
        "\t\t\tself.label = np.argmax(self.classes)\n",
        " \n",
        "\t\treturn self.label\n",
        " \n",
        "\tdef get_score(self):\n",
        "\t\tif self.score == -1:\n",
        "\t\t\tself.score = self.classes[self.get_label()]\n",
        " \n",
        "\t\treturn self.score\n",
        "\n",
        "def _sigmoid(x):\n",
        "\treturn 1. / (1. + np.exp(-x))\n",
        " \n",
        "def decode_netout(netout, anchors, net_h, net_w):\n",
        "\tgrid_h, grid_w = netout.shape[:2]\n",
        "\tnb_box = 3\n",
        "\tnetout = netout.reshape((grid_h, grid_w, nb_box, -1))\n",
        "\tnb_class = netout.shape[-1] - 5\n",
        "\tboxes = []\n",
        "\tnetout[..., :2]  = _sigmoid(netout[..., :2])\n",
        "\tnetout[..., 4:]  = _sigmoid(netout[..., 4:])\n",
        "\tnetout[..., 5:]  = netout[..., 4][..., np.newaxis] * netout[..., 5:]\n",
        " \n",
        "\tfor i in range(grid_h*grid_w):\n",
        "\t\trow = i / grid_w\n",
        "\t\tcol = i % grid_w\n",
        "\t\tfor b in range(nb_box):\n",
        "\t\t\t# 4th element is objectness score\n",
        "\t\t\tobjectness = netout[int(row)][int(col)][b][4]\n",
        "\t\t\t# if(objectness.all() <= obj_thresh): continue\n",
        "\t\t\t# first 4 elements are x, y, w, and h\n",
        "\t\t\tx, y, w, h = netout[int(row)][int(col)][b][:4]\n",
        "\t\t\tx = (col + x) / grid_w # center position, unit: image width\n",
        "\t\t\ty = (row + y) / grid_h # center position, unit: image height\n",
        "\t\t\tw = anchors[2 * b + 0] * np.exp(w) / net_w # unit: image width\n",
        "\t\t\th = anchors[2 * b + 1] * np.exp(h) / net_h # unit: image height\n",
        "\t\t\t# last elements are class probabilities\n",
        "\t\t\tclasses = netout[int(row)][col][b][5:]\n",
        "\t\t\tbox = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, objectness, classes)\n",
        "\t\t\tboxes.append(box)\n",
        "\treturn boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "VcC3a0tjtYlv"
      },
      "outputs": [],
      "source": [
        "anchors = [[116,90, 156,198, 373,326], [30,61, 62,45, 59,119], [10,13, 16,30, 33,23]]\n",
        "boxes = list()\n",
        "for i in range(len(yhat)):\n",
        "\tboxes += decode_netout(yhat[i][0], anchors[i], net_h=IMAGE_HEIGHT, net_w=IMAGE_WIDTH)\n",
        "\n",
        "for i in range(len(boxes)):\n",
        "\tx_offset, x_scale = (IMAGE_WIDTH - IMAGE_WIDTH)/2./IMAGE_HEIGHT, float(IMAGE_WIDTH)/IMAGE_WIDTH\n",
        "\ty_offset, y_scale = (IMAGE_HEIGHT - IMAGE_HEIGHT)/2./IMAGE_HEIGHT, float(IMAGE_HEIGHT)/IMAGE_HEIGHT\n",
        "\tboxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)\n",
        "\tboxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)\n",
        "\tboxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)\n",
        "\tboxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "4BVYDz-UBGFP",
        "outputId": "0b35942c-8104-4b5a-b477-e3b060182d98"
      },
      "outputs": [],
      "source": [
        "len(boxes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cX39d08LBMNu"
      },
      "source": [
        "## Boxes Filters\n",
        "\n",
        "From the sections above, it is clear to see that there are over ten thousands of boxes detected. This next step is to filter the boxes with very low confidence by using threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "tghx5DRpr3hn"
      },
      "outputs": [],
      "source": [
        "def box_filter(boxes,labels,threshold_socre):\n",
        "\tvalid_boxes=[]\n",
        "\tvalid_labels=[]\n",
        "\tvalid_scores=[]\n",
        "\tfor box in boxes:\n",
        "\t\tfor i in range(len(labels)):\n",
        "\t\t\tif box.classes[i] > threshold_socre:\n",
        "\t\t\t\tvalid_boxes.append(box)\n",
        "\t\t\t\tvalid_labels.append(labels[i])\n",
        "\t\t\t\tvalid_scores.append(box.classes[i])\n",
        "\t\t\n",
        "\treturn (valid_boxes,valid_labels,valid_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "E7ewYu44umZ6"
      },
      "outputs": [],
      "source": [
        "valid_data= box_filter(boxes, labels, threshold_socre=0.6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FIu6fLbu0l6b"
      },
      "source": [
        "## Draw all the boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "OpCz8tYK0sh6"
      },
      "outputs": [],
      "source": [
        "def draw_boxes(filename, valid_data):\n",
        "\n",
        "\tdata = pyplot.imread(filename)\n",
        "\tpyplot.imshow(data)\n",
        "\tax = pyplot.gca()\n",
        "\tfor i in range(len(valid_data[0])):\n",
        "\t\tbox = valid_data[0][i]\n",
        "\t\ty1, x1, y2, x2 = box.ymin, box.xmin, box.ymax, box.xmax\n",
        "\t\twidth, height = x2 - x1, y2 - y1\n",
        "\t\trect = Rectangle((x1, y1), width, height, fill=False, color='white')\n",
        "\t\tax.add_patch(rect)\n",
        "\t\tprint(valid_data[1][i], valid_data[2][i])\n",
        "\t\tlabel = \"%s (%.3f)\" % (valid_data[1][i], valid_data[2][i])\n",
        "\t\tpyplot.text(x1, y1, label, color='white')\n",
        "\tpyplot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        },
        "colab_type": "code",
        "id": "K1KeBbGLuuQL",
        "outputId": "1bc26826-1fb7-45be-ee65-96d7f6d3d47a"
      },
      "outputs": [],
      "source": [
        "draw_boxes(photo_filename,valid_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vj1x3pGVFqgB"
      },
      "source": [
        "## Non-max Seppression, NMS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "d9HqPhnkT6oZ"
      },
      "outputs": [],
      "source": [
        "def encoder_dic(valid_data):\n",
        "  data_dic={}\n",
        "  (valid_boxes,valid_labels,valid_scores)=valid_data\n",
        "  for box, label,score in zip(valid_boxes,valid_labels,valid_scores):\n",
        "    if label not in data_dic:\n",
        "      data_dic[label]=[[score,box,'kept']]\n",
        "    else:\n",
        "      data_dic[label].append([score,box,'kept'])\n",
        "      \n",
        "  return data_dic\n",
        "dic=encoder_dic(valid_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_kLp8wd6FfP3"
      },
      "outputs": [],
      "source": [
        "def decode_box_coor(box):\n",
        "  return (box.xmin, box.ymin,box.xmax, box.ymax )\n",
        "\n",
        "def iou(box1, box2):\n",
        "  (box1_x1, box1_y1, box1_x2, box1_y2) = decode_box_coor(box1)\n",
        "  (box2_x1, box2_y1, box2_x2, box2_y2) = decode_box_coor(box2)\n",
        "\n",
        "  xi1 = max(box1_x1,box2_x1)\n",
        "  yi1 = max(box1_y1,box2_y1)\n",
        "  xi2 = min(box1_x2,box2_x2)\n",
        "  yi2 = min(box1_y2,box2_y2)\n",
        "  inter_width = xi2-xi1\n",
        "  inter_height = yi2-yi1\n",
        "  inter_area = max(inter_height,0)*max(inter_width,0)\n",
        "\n",
        "  box1_area = (box1_x2-box1_x1)*(box1_y2-box1_y1)\n",
        "  box2_area = (box2_x2-box2_x1)*(box2_y2-box2_y1)\n",
        "  union_area = box1_area+box2_area-inter_area \n",
        "\n",
        "  iou = inter_area/union_area\n",
        "  \n",
        "  return iou\n",
        "\n",
        "def do_nms(data_dic, nms_thresh):\n",
        "  final_boxes,final_scores,final_labels=list(),list(),list()\n",
        "  for label in data_dic:\n",
        "    scores_boxes=sorted(data_dic[label],reverse=True)\n",
        "    for i in range(len(scores_boxes)):\n",
        "      if scores_boxes[i][2]=='removed': continue\n",
        "      for j in range(i+1,len(scores_boxes)):\n",
        "        if iou(scores_boxes[i][1],scores_boxes[j][1]) >= nms_thresh:\n",
        "          scores_boxes[j][2]=\"removed\"\n",
        "\n",
        "    for e in scores_boxes:\n",
        "      print(label+' '+str(e[0]) + \" status: \"+ e[2])\n",
        "      if e[2]=='kept':\n",
        "        final_boxes.append(e[1])\n",
        "        final_labels.append(label)\n",
        "        final_scores.append(e[0])\n",
        "    \n",
        "\n",
        "  return (final_boxes,final_labels,final_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "colab_type": "code",
        "id": "7tKq4lZYiLfV",
        "outputId": "a3796a0a-a78e-4136-9ab9-d76ec8a42362"
      },
      "outputs": [],
      "source": [
        "final_data=do_nms(dic, 0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "colab_type": "code",
        "id": "L6TTEa14iwKJ",
        "outputId": "b1e1198e-58fb-43b4-dc24-ff4dfa18f0de"
      },
      "outputs": [],
      "source": [
        "draw_boxes(photo_filename,final_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FP6jc8D0lmDu"
      },
      "source": [
        "### Use Keras \"non_max_suppression\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "6krAd4QxImK4"
      },
      "outputs": [],
      "source": [
        "def yolo_non_max_suppression(scores, boxes, classes, max_boxes = 10, iou_threshold = 0.5):\n",
        "    \n",
        "    max_boxes_tensor = K.variable(max_boxes, dtype='int32')     # tensor to be used in tf.image.non_max_suppression()\n",
        "    K.get_session().run(tf.variables_initializer([max_boxes_tensor])) # initialize variable max_boxes_tensor\n",
        "    nms_indices = tf.image.non_max_suppression(scores=scores,boxes=boxes,max_output_size=max_boxes,iou_threshold=iou_threshold)\n",
        "\n",
        "    scores = K.gather(scores,nms_indices)\n",
        "    boxes = K.gather(boxes,nms_indices)\n",
        "    classes = K.gather(classes,nms_indices)\n",
        "\n",
        "    return scores, boxes, classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kwYVtNF4l41m"
      },
      "source": [
        "## Other Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "k-KsBFOSl4Ps"
      },
      "outputs": [],
      "source": [
        "def showresults(path):\n",
        "  _image,width,height=load_and_preprocess_image(path,[IMAGE_WIDTH,IMAGE_HEIGHT])\n",
        "  image = expand_dims(_image, 0)\n",
        "  yhat = model.predict(image)\n",
        "  boxes = list()\n",
        "  for i in range(len(yhat)):\n",
        "\t  boxes += decode_netout(yhat[i][0], anchors[i], net_h=IMAGE_HEIGHT, net_w=IMAGE_WIDTH)\n",
        "  for i in range(len(boxes)):\n",
        "    x_offset, x_scale = (IMAGE_WIDTH - IMAGE_WIDTH)/2./IMAGE_HEIGHT, float(IMAGE_WIDTH)/IMAGE_WIDTH\n",
        "    y_offset, y_scale = (IMAGE_HEIGHT - IMAGE_HEIGHT)/2./IMAGE_HEIGHT, float(IMAGE_HEIGHT)/IMAGE_HEIGHT\n",
        "    boxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)\n",
        "    boxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)\n",
        "    boxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)\n",
        "    boxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)\n",
        "  valid_data= box_filter(boxes, labels, threshold_socre=0.6)\n",
        "  dic=encoder_dic(valid_data)\n",
        "  final_data=do_nms(dic, 0.7)\n",
        "  draw_boxes(path,final_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "colab_type": "code",
        "id": "8BE3CN9QwzNd",
        "outputId": "4216e2be-1439-450f-8d75-cea804f60ee5"
      },
      "outputs": [],
      "source": [
        "showresults('/content/drive/My Drive/DPprojects/Object Detection - Yolo/images/zebra.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "colab_type": "code",
        "id": "GEeGlbEeyE0T",
        "outputId": "14c30bd1-91de-4b0d-d7a2-d8a7b6f62dc5"
      },
      "outputs": [],
      "source": [
        "showresults('/content/drive/My Drive/DPprojects/Object Detection - Yolo/images/kangaroo.png')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "LozuraZZ_wnj"
      ],
      "include_colab_link": true,
      "name": "Object Detection - Yolo.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
